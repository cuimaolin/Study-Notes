数据的分类（今日头条测开二面）

非关系型数据库有哪些（今日头条测开二面）

> [非关系型数据库有哪些](https://www.huaweicloud.com/zhishi/db21.html)
> 
> - 文档数据库——这些数据库通常将每个键与称为文档的复杂数据结构配对。文档可以包含键数组对、键值对甚至嵌套文档。包括：
>   - MongoDB等
> - 键值存储——每个单独的项都存储为键值对。键值存储是所有NoSQL数据库中最简单的数据库
>   - Redis、Memcached等
> - 宽列存储——这些类型的数据库针对大型数据集上的查询进行了优化，它们将数据列存储在一起，而不是行
>   - Cassandra、Hbase等
> - 图形存储——这些存储关于图形、网络的信息，例如社会关系、路线图、交通链接
>   - Neo4j、AllegroGraph

### 5.1 MySQL

MySQL的索引（淘宝一面，腾讯云二面，==腾讯TEG应用开发一面==，==超参数一面==）

> [MySQL索引-JavaGuide](https://snailclimb.gitee.io/javaguide/#/docs/database/mysql/MySQL%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B4%A2%E5%BC%95)
> 
> 一、何为索引？有什么作用
> 
> 索引是一种用于快速查询和检索数据的数据结构。常见的索引结构有: B 树， B+树和 Hash。
> 
> 索引的作用就相当于目录的作用。打个比方: 我们在查字典的时候，如果没有目录，那我们就只能一页一页的去找我们需要查的那个字，速度很慢。如果有目录了，我们只需要先去目录里查找字的位置，然后直接翻到那一页就行了。
> 
> 二、索引的优缺点
> 
> 1、优点：
> 
> - 使用索引可以大大加快 数据的检索速度（大大减少检索的数据量）, 这也是创建索引的最主要的原因。
> - 通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性。
> 
> 2、缺点：
> 
> - 创建索引和维护索引需要耗费许多时间。当对表中的数据进行增删改的时候，如果数据有索引，那么索引也需要动态的修改，会降低 SQL 执行效率。
> - 索引需要使用物理文件存储，也会耗费一定空间。
> 
> 三、索引的底层数据结构
> 
> 1、为什么选择B+树不选择Hash表
> 
> - Hash 冲突问题 ：我们上面也提到过Hash 冲突了，不过对于数据库来说这还不算最大的缺点。
> 
> - Hash 索引不支持顺序和范围查询(Hash 索引不支持顺序和范围查询
> 
> 2、B树和B+树的区别
> 
> - B 树的所有节点既存放键(key) 也存放 数据(data)，而 B+树只有叶子节点存放 key 和 data，其他内节点只存放 key。
> - B 树的叶子节点都是独立的;B+树的叶子节点有一条引用链指向与它相邻的叶子节点。
> - B 树的检索的过程相当于对范围内的每个节点的关键字做二分查找，可能还没有到达叶子节点，检索就结束了。而 B+树的检索效率就很稳定了，任何查找都是从根节点到叶子节点的过程，叶子节点的顺序检索很明显。
> 
> 3、MyISAM索引的底层结构
> 
> - B+Tree 叶节点的 data 域存放的是数据记录的地址
> - 在索引检索的时候，首先按照 B+Tree 搜索算法搜索索引，如果指定的 Key 存在，则取出其 data 域的值，然后以 data 域的值为地址读取相应的数据记录
> - 这被称为“非聚簇索引”
> 
> 4、InnoDB索引的底层结构
> 
> - InnoDB 引擎中，其表数据文件本身就是按 B+Tree 组织的一个索引结构，树的叶节点 data 域保存了完整的数据记录
> - 这个索引的 key 是数据表的主键，因此 InnoDB 表数据文件本身就是主索引，这被称为“聚簇索引（或聚集索引）”
> - 其余的索引都作为辅助索引，辅助索引的 data 域存储相应记录主键的值而不是地址
> - 在根据主索引搜索时，直接找到 key 所在的节点即可取出数据；在根据辅助索引查找时，则需要先取出主键的值，在走一遍主索引
> - 在设计表的时候，不建议使用过长的字段作为主键，也不建议使用非单调的字段作为主键，这样会造成主索引频繁分裂
> 
> 5、B+树为什么适合磁盘存储
> 
> [B+树在磁盘存储中的应用](https://www.cnblogs.com/nullzx/p/8978177.html)
> 
> - 主存和磁盘之间的数据交换不是以字节为单位的，而是以n个扇区为单位的（一个扇区有512字节）
> - 假设，我们现在选择4KB作为内存和磁盘之间的传输单位，那么我们在设计B+树的时候，不论是索引结点还是叶子结点都使用4KB作为结点的大小
> - 快速的原因是，索引结点中不存数据，只存键和指针，所以一个索引结点就可以存储大量的分支，而一个索引结点只需要一次IO即可读取到内存中。
> 
> 四、主键索引与辅助索引
> 
> 1、主键索引。数据表的主键列使用的就是主键索引
> 
> 2、二级索引又称为辅助索引，是因为二级索引的叶子节点存储的数据是主键。也就是说，通过二级索引，可以定位主键的位置。
> 
> - 唯一索引(Unique Key) ：唯一索引也是一种约束。唯一索引的属性列不能出现重复的数据，但是允许数据为 NULL，一张表允许创建多个唯一索引。建立唯一索引的目的大部分时候都是为了该属性列的数据的唯一性，而不是为了查询效率。
> - 普通索引(Index) ：普通索引的唯一作用就是为了快速查询数据，一张表允许创建多个普通索引，并允许数据重复和 NULL。
> - 前缀索引(Prefix)：前缀索引只适用于字符串类型的数据。前缀索引是对文本的前几个字符创建索引，相比普通索引建立的数据更小， 因为只取前几个字符。
> - 全文索引(Full Text) ：全文索引主要是为了检索大文本数据中的关键字的信息，是目前搜索引擎数据库使用的一种技术。Mysql5.6 之前只有 MYISAM 引擎支持全文索引，5.6 之后 InnoDB 也支持了全文索引。
> 
> 五、聚集索引和非聚集索引
> 
> 1、聚集索引
> 
> 聚集索引即索引结构和数据一起存放的索引。主键索引属于聚集索引。
> 
> 优点
> 
> - 聚集索引的查询速度非常的快，因为整个 B+树本身就是一颗多叉平衡树，叶子节点也都是有序的，定位到索引的节点，就相当于定位到了数据。
> 
> 缺点
> 
> - 依赖于有序的数据 ：因为 B+树是多路平衡树，如果索引的数据不是有序的，那么就需要在插入时排序，如果数据是整型还好，否则类似于字符串或 UUID 这种又长又难比较的数据，插入或查找的速度肯定比较慢。
> 
> - 更新代价大 ： 如果对索引列的数据被修改时，那么对应的索引也将会被修改， 而且况聚集索引的叶子节点还存放着数据，修改代价肯定是较大的， 所以对于主键索引来说，主键一般都是不可被修改的。
> 
> 2、非聚集索引
> 
> 非聚集索引即索引结构和数据分开存放的索引
> 
> 优点
> 
> - 更新代价比聚集索引要小 。非聚集索引的更新代价就没有聚集索引那么大了，非聚集索引的叶子节点是不存放数据的
> 
> 缺点
> 
> - 跟聚集索引一样，非聚集索引也依赖于有序的数据
> - 可能会二次查询(回表):这应该是非聚集索引最大的缺点了。 当查到索引对应的指针或主键后，可能还需要根据指针或主键再到数据文件或表中查询。
> 
> 六、覆盖索引
> 
> 覆盖索引即需要查询的字段正好是索引的字段，那么直接根据该索引，就可以查到数据了， 而无需回表查询。
> 
> - 如主键索引，如果一条 SQL 需要查询主键，那么正好根据主键索引就可以查到主键。
> 
> - 再如普通索引，如果一条 SQL 需要查询 name，name 字段正好有索引， 那么直接根据这个索引就可以查到数据，也无需回表。
> 
> 七、创建索引的注意事项
> 
> 1、选择合适的字段创建索引
> 
> - 不为 NULL 的字段 ：索引字段的数据应该尽量不为 NULL，因为对于数据为 NULL 的字段，数据库较难优化。如果字段频繁被查询，但又避免不了为 NULL，建议使用 0,1,true,false 这样语义较为清晰的短值或短字符作为替代。
> - 被频繁查询的字段 ：我们创建索引的字段应该是查询操作非常频繁的字段。
> - 被作为条件查询的字段 ：被作为 WHERE 条件查询的字段，应该被考虑建立索引。
> - 频繁需要排序的字段：索引已经排序，这样查询可以利用索引的排序，加快排序查询时间。
> - 被经常频繁用于连接的字段：经常用于连接的字段可能是一些外键列，对于外键列并不一定要建立外键，只是说该列涉及到表与表的关系。对于频繁被连接查询的字段，可以考虑建立索引，提高多表连接查询的效率。
> 
> 2、被频繁更新的字段应该慎重建立索引
> 
> 虽然索引能带来查询上的效率，但是维护索引的成本也是不小的。 如果一个字段不被经常查询，反而被经常修改，那么就更不应该在这种字段上建立索引了。
> 
> 3、尽可能地考虑建立联合索引而不是单列索引
> 
> 因为索引是需要占用磁盘空间的，可以简单理解为每个索引都对应着一颗 B+树。如果一个表的字段过多，索引过多，那么当这个表的数据达到一个体量后，索引占用的空间也是很多的，且修改索引时，耗费的时间也是较多的。如果是联合索引，多个字段在一个索引上，那么将会节约很大磁盘空间，且修改数据的操作效率也会提升。
> 
> 4、注意避免冗余索引
> 
> 冗余索引指的是索引的功能相同，能够命中索引(a, b)就肯定能命中索引(a) ，那么索引(a)就是冗余索引。如（name,city ）和（name ）这两个索引就是冗余索引，能够命中前者的查询肯定是能够命中后者的 在大多数情况下，都应该尽量扩展已有的索引而不是创建新索引。
> 
> 5、考虑在字符串类型地字段上使用前缀索引代替普通索引
> 
> 前缀索引仅限于字符串类型，较普通索引会占用更小的空间，所以可以考虑使用前缀索引带替普通索引。
> 
> 七、使用索引的一些建议
> 
> - 对于中到大型表索引都是非常有效的，但是特大型表的话维护开销会很大，不适合建索引
> - 避免 where 子句中对字段施加函数，这会造成无法命中索引。
> - 在使用 InnoDB 时使用与业务无关的自增主键作为主键，即使用逻辑主键，而不要使用业务主键。
> - 删除长期未使用的索引，不用的索引的存在会造成不必要的性能损耗
> - 在使用 limit offset 查询缓慢时，可以借助索引来提高性能

MyIASM和InnoDB的区别（==百度一面==）

> - MyISAM 只有表级锁(table-level locking)，而 InnoDB 支持行级锁(row-level locking)和表级锁,默认为行级锁。
> 
> - MyISAM 不提供事务支持。InnoDB 提供事务支持，具有提交(commit)和回滚(rollback)事务的能力。
> 
> - MyISAM 不支持外键，而 InnoDB 支持。
> 
> - MyISAM 不支持数据库异常崩溃后的安全恢复，而 InnoDB 支持。使用 InnoDB 的数据库在异常崩溃后，数据库重新启动的时候会保证数据库恢复到崩溃前的状态。这个恢复的过程依赖于 `redo log` 。
> 
> - MyISAM 不支持MVCC，而 InnoDB 支持MVCC。

InnoDB事务的四大隔离级别（腾讯云一面，==百度一面==）

> 一、ACID特性
> 
> 1. 原子性（`Atomicity`） ： 事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用；
> 2. 一致性（`Consistency`）： 执行事务前后，数据保持一致，例如转账业务中，无论事务是否成功，转账者和收款人的总额应该是不变的；
> 3. 隔离性（`Isolation`）： 并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的；
> 4. 持久性（`Durability`）： 一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。
> 
> 二、数据事务的实现原理
> 
> MySQL InnoDB 引擎使用 **redo log(重做日志)** 保证事务的**持久性**，使用 **undo log(回滚日志)** 来保证事务的**原子性**。
> 
> MySQL InnoDB 引擎通过 **锁机制**、**MVCC** 等手段来保证事务的隔离性（ 默认支持的隔离级别是 **`REPEATABLE-READ`** ）。
> 
> 保证了事务的持久性、原子性、隔离性之后，一致性才能得到保障。
> 
> 三、并发事务带来哪些问题
> 
> - 脏读（Dirty read）: 当一个事务正在访问数据并且对数据进行了修改，而这种修改还没有提交到数据库中，这时另外一个事务也访问了这个数据，然后使用了这个数据。因为这个数据是还没有提交的数据，那么另外一个事务读到的这个数据是“脏数据”，依据“脏数据”所做的操作可能是不正确的。
> - 丢失修改（Lost to modify）: 指在一个事务读取一个数据时，另外一个事务也访问了该数据，那么在第一个事务中修改了这个数据后，第二个事务也修改了这个数据。这样第一个事务内的修改结果就被丢失，因此称为丢失修改。 例如：事务 1 读取某表中的数据 A=20，事务 2 也读取 A=20，事务 1 修改 A=A-1，事务 2 也修改 A=A-1，最终结果 A=19，事务 1 的修改被丢失。
> - 不可重复读（Unrepeatable read）: 指在一个事务内多次读同一数据。在这个事务还没有结束时，另一个事务也访问该数据。那么，在第一个事务中的两次读数据之间，由于第二个事务的修改导致第一个事务两次读取的数据可能不太一样。这就发生了在一个事务内两次读到的数据是不一样的情况，因此称为不可重复读。
> - 幻读（Phantom read）: 幻读与不可重复读类似。它发生在一个事务（T1）读取了几行数据，接着另一个并发事务（T2）插入了一些数据时。在随后的查询中，第一个事务（T1）就会发现多了一些原本不存在的记录，就好像发生了幻觉一样，所以称为幻读。
> 
> 四、事务的隔离级别
> 
> - READ-UNCOMMITTED(读取未提交)：最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读。
> - READ-COMMITTED(读取已提交)：允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生。
> - REPEATABLE-READ(可重复读)：对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。
> - SERIALIZABLE(可串行化)：最高的隔离级别，完全服从 ACID 的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。
> 
> MySQL InnoDB 的 REPEATABLE-READ（可重读）并不保证避免幻读，需要应用使用加锁读来保证。而这个加锁度使用到的机制就是 Next-Key Locks。
> 
> [MySQL的多版本并发控制(MVCC)是什么？](https://segmentfault.com/a/1190000037557620)

MySQL的锁（腾讯云一面，==腾讯CDG事务开发一面==）

> Record lock行锁
> 
> - 单个行记录上的锁
> 
> Gap lock间隙锁
> 
> - 锁定一个范围，不包括记录本身
> - 间隙锁的目的是为了阻止多个事务将记录插入同一范围内，这样会导致幻读
> 
> Next-key
> 
> - 行锁+间隙锁 锁定一个范围，包含记录本身

数据库中乐观锁和悲观锁。数据库中的悲观锁的语法。（腾讯CDG一面）

> [MySQL/InnoDB中，乐观锁、悲观锁、共享锁、排它锁、行锁、表锁、死锁概念的理解](https://segmentfault.com/a/1190000015815061)
> 
> 一、乐观锁
> 
> - 用数据版本(version)记录机制实现
> 
> - 为数据增加一个版本共识，一般是通过为数据库表增加一个数据类型的“version”字段来实现
> 
> - 当读取数据时，将version字段的值一同读出，数据每更新一次，对此version值加1.
> 
> - 当提交更新的时候，判断数据表对应记录的当前版本信息与第一次取出来的version值进行对比
>   
>   - 如果数据库表当前版本号与第一次取出来的version值相等，则更新，否则认为是过期数据
> 
> - 数据库表设计
>   
>   - 三个字段分别是id, value, version
>   - `select id,value,version from TABLE where id=#{id}`
> 
> - 每次更新表中的value字段时，为了防止冲突，需要这样操作
>   
>   ```
>   update TABLE
>   set value=2,version=version+1
>   where id=#{id} and version=#{version};
>   ```
> 
> 二、悲观锁
> 
> - 悲观锁就是在操作数据时，认为此操作会出现数据冲突，所以在进行每次操作时都要通过获取锁才能进行对相同数据的操作
> - 共享锁与排它锁时悲观锁的两种不同实现
> - 要使用悲观锁，我们必须关闭MySQL数据库的自动提交属性
> 
> ```mysql
> set autocommit=0;
> # 设置完autocommit后，我们就可以执行我们的正常业务了。具体如下：
> # 1. 开始事务
> begin;/begin work;/start transaction; (三者选一就可以)
> # 2. 查询表信息
> select status from TABLE where id=1 for update;
> # 3. 插入一条数据
> insert into TABLE (id,value) values (2,2);
> # 4. 修改数据为
> update TABLE set value=2 where id=1;
> # 5. 提交事务
> commit;/commit work;
> ```

后端如何优化数据库性能？（淘宝一面，==百度一面==）

> 一、使用Explain进行分析
> 
> - Explain用来分析SELECT查询语句，开发人员可以通过Explain结果来优化查询语句
> - 比较重要的字段：
>   - select_type：查询类型，有简单查询、联合查询、子查询等
>   - key：使用的索引
>   - rows：扫描的行数
> 
> 二、优化数据访问
> 
> - 减少请求的数据量
>   - 只返回必要的列：最好不要使用SELECT *语句
>   - 只返回必要的行：使用LIMIT语句来限制返回的数据。
>   - 缓存重复查询的数据：使用缓存可以避免在数据库中进行查询，特别在要查询的数据经常被重复查询时，缓存带来的查询性能提升将会是非常明显的。
> - 减少服务端扫描的行数
>   - 最有效的方法是使用索引来覆盖查询
> 
> 三、重构查询方式
> 
> - 切分大查询
>   - 一个大查询如果一次性执行的话，可能一次锁住很多数据、占满整个事务日志、耗尽系统资源、阻塞很多小的但重要的查询。
> - 分解大连接查询
>   - 将一个大连接查询分解成对每一个表进行一次单表查询，然后再应用程序中关联。这样做的好处有：
>     - 让缓存更高效。对于连接查询，如果其中一个表发生变化，那么整个查询缓存就无法使用。而分解后的多个查询，即使其中一个表发生变化，对其它表的查询缓存依然可以使用。分解成多个单表查询，这些单表查询的缓存结果更可能被其它查询使用到，从而减少冗余记录的查询。
>     - 减少锁竞争；
>     - 在应用层进行连接，可以更容易对数据库进行拆分，从而更容易做到高性能和可伸缩。
>     - 查询本身效率也可能会有所提升。

主从分离中主服务器和从服务器如何同步？MySQL的分布式（淘宝一面，==Lazada二面==，==超参数一面==）

> 一、主从复制
> 
> 主要涉及三个线程：binlog 线程、I/O 线程和 SQL 线程。
> 
> - **binlog 线程** ：负责将主服务器上的数据更改写入二进制日志（Binary log）中。
> - **I/O 线程** ：负责从主服务器上读取二进制日志，并写入从服务器的中继日志（Relay log）。
> - **SQL 线程** ：负责读取中继日志，解析出主服务器已经执行的数据更改并在从服务器中重放（Replay）。
> 
> ![](https://cs-notes-1256109796.cos.ap-guangzhou.myqcloud.com/master-slave.png)
> 
> 二、读写分离
> 
> 主服务器处理写操作以及实时性要求比较高的读操作，而从服务器处理读操作。
> 
> 读写分离能提高性能的原因在于：
> 
> - 主从服务器负责各自的读和写，极大程度缓解了锁的争用；
> - 从服务器可以使用 MyISAM，提升查询性能以及节约系统开销；
> - 增加冗余，提高可用性。
> 
> 读写分离常用代理方式来实现，代理服务器接收应用层传来的读写请求，然后决定转发到哪个服务器。
> 
> ![img](https://camo.githubusercontent.com/7f9279aeb3dd23a8a0a64895594bd76ac9fce2dfb6bc24974a07cc83888c6fc9/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f6d61737465722d736c6176652d70726f78792e706e67)

MySQL语法（腾讯CDG一面，==腾讯CDG事务开发一面==，==百度一面==）

> [一千行 MySQL 学习笔记](https://snailclimb.gitee.io/javaguide/#/docs/database/mysql/%E4%B8%80%E5%8D%83%E8%A1%8CMySQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0)
> 
> 事务（腾讯CDG一面）
> 
> ```mysql
> -- 事务开启
>     START TRANSACTION; 或者 BEGIN;
>     开启事务后，所有被执行的SQL语句均被认作当前事务内的SQL语句。
> -- 事务提交
>     COMMIT;
> -- 事务回滚
>     ROLLBACK;
>     如果部分操作发生问题，映射到事务开启前。
> ```
> 
> SELECT（腾讯CDG一面）
> 
> ```mysql
> /* SELECT */ ------------------
> SELECT [ALL|DISTINCT] select_expr FROM -> WHERE -> GROUP BY [合计函数] -> HAVING -> ORDER BY -> LIMIT
> a. select_expr
>     -- 可以用 * 表示所有字段。
>         select * from tb;
>     -- 可以使用表达式（计算公式、函数调用、字段也是个表达式）
>         select stu, 29+25, now() from tb;
>     -- 可以为每个列使用别名。适用于简化列标识，避免多个列标识符重复。
>         - 使用 as 关键字，也可省略 as.
>         select stu+10 as add10 from tb;
> b. FROM 子句
>     用于标识查询来源。
>     -- 可以为表起别名。使用as关键字。
>         SELECT * FROM tb1 AS tt, tb2 AS bb;
>     -- from子句后，可以同时出现多个表。
>         -- 多个表会横向叠加到一起，而数据会形成一个笛卡尔积。
>         SELECT * FROM tb1, tb2;
>     -- 向优化符提示如何选择索引
>         USE INDEX、IGNORE INDEX、FORCE INDEX
>         SELECT * FROM table1 USE INDEX (key1,key2) WHERE key1=1 AND key2=2 AND key3=3;
>         SELECT * FROM table1 IGNORE INDEX (key3) WHERE key1=1 AND key2=2 AND key3=3;
> c. WHERE 子句
>     -- 从from获得的数据源中进行筛选。
>     -- 整型1表示真，0表示假。
>     -- 表达式由运算符和运算数组成。
>         -- 运算数：变量（字段）、值、函数返回值
>         -- 运算符：
>             =, <=>, <>, !=, <=, <, >=, >, !, &&, ||,
>             in (not) null, (not) like, (not) in, (not) between and, is (not), and, or, not, xor
>             is/is not 加上ture/false/unknown，检验某个值的真假
>             <=>与<>功能相同，<=>可用于null比较
> d. GROUP BY 子句, 分组子句
>     GROUP BY 字段/别名 [排序方式]
>     分组后会进行排序。升序：ASC，降序：DESC
>     以下[合计函数]需配合 GROUP BY 使用：
>     count 返回不同的非NULL值数目  count(*)、count(字段)
>     sum 求和
>     max 求最大值
>     min 求最小值
>     avg 求平均值
>     group_concat 返回带有来自一个组的连接的非NULL值的字符串结果。组内字符串连接。
> e. HAVING 子句，条件子句
>     与 where 功能、用法相同，执行时机不同。
>     where 在开始时执行检测数据，对原数据进行过滤。
>     having 对筛选出的结果再次进行过滤。
>     having 字段必须是查询出来的，where 字段必须是数据表存在的。
>     where 不可以使用字段的别名，having 可以。因为执行WHERE代码时，可能尚未确定列值。
>     where 不可以使用合计函数。一般需用合计函数才会用 having
>     SQL标准要求HAVING必须引用GROUP BY子句中的列或用于合计函数中的列。
> f. ORDER BY 子句，排序子句
>     order by 排序字段/别名 排序方式 [,排序字段/别名 排序方式]...
>     升序：ASC，降序：DESC
>     支持多个字段的排序。
> g. LIMIT 子句，限制结果数量子句
>     仅对处理好的结果进行数量限制。将处理好的结果的看作是一个集合，按照记录出现的顺序，索引从0开始。
>     limit 起始位置, 获取条数
>     省略第一个参数，表示从索引0开始。limit 获取条数
> h. DISTINCT, ALL 选项
>     distinct 去除重复记录
>     默认为 all, 全部记录
> ```
> 
> 连接查询（==百度一面==）
> 
> ```mysql
> /* 连接查询(join) */ ------------------
>     将多个表的字段进行连接，可以指定连接条件。
> -- 内连接(inner join)
>     - 默认就是内连接，可省略inner。
>     - 只有数据存在时才能发送连接。即连接结果不能出现空行。
>     on 表示连接条件。其条件表达式与where类似。也可以省略条件（表示条件永远为真）
>     也可用where表示连接条件。
>     还有 using, 但需字段名相同。 using(字段名)
>     -- 交叉连接 cross join
>         即，没有条件的内连接。
>         select * from tb1 cross join tb2;
> -- 外连接(outer join)
>     - 如果数据不存在，也会出现在连接结果中。
>     -- 左外连接 left join
>         如果数据不存在，左表记录会出现，而右表为null填充
>     -- 右外连接 right join
>         如果数据不存在，右表记录会出现，而左表为null填充
> -- 自然连接(natural join)
>     自动判断连接条件完成连接。
>     相当于省略了using，会自动查找相同字段名。
>     natural join
>     natural left join
>     natural right join
> select info.id, info.name, info.stu_num, extra_info.hobby, extra_info.sex from info, extra_info where info.stu_num = extra_info.stu_id;
> ```
> 
> UNION（==百度一面==）
> 
> ```mysql
> /* UNION */ ------------------
>     将多个select查询的结果组合成一个结果集合。
>     SELECT ... UNION [ALL|DISTINCT] SELECT ...
>     默认 DISTINCT 方式，即所有返回的行都是唯一的
>     建议，对每个SELECT查询加上小括号包裹。
>     ORDER BY 排序时，需加上 LIMIT 进行结合。
>     需要各select查询的字段数量一样。
>     每个select查询的字段列表(数量、类型)应一致，因为结果中的字段名以第一条select语句为准。
> ```
> 
> 锁表（==腾讯CDG事务开发一面==）
> 
> ```mysql
> /* 锁表 */
> 表锁定只用于防止其它客户端进行不正当地读取和写入
> MyISAM 支持表锁，InnoDB 支持行锁
> -- 锁定
>     LOCK TABLES tbl_name [AS alias]
> -- 解锁
>     UNLOCK TABLES
> ```

### 5.2 Redis

Redis为什么那么快？（淘宝一面，腾讯云二面，==腾讯CDG事务开发一面==）

> [一文搞懂 Redis 高性能之 IO 多路复用](https://xie.infoq.cn/article/b3816e9fe3ac77684b4f29348)
> 
> 1. Redis完全基于内存，绝大部分请求是纯粹的内存操作，非常迅速
> 2. 数据结构简单，对数据的操作也简单；
> 3. 采用单线程，避免了不必要的上下文切换和竞争条件；
> 4. 使用多路复用IO模型，不在IO上浪费时间

Redis和Memcache的区别（今日头条测开二面）

> 一、共同点
> 
> 1. 都是基于内存的数据库，一般都用来当做缓存使用。
> 2. 都有过期策略。
> 3. 两者的性能都非常高。
> 
> 二、区别
> 
> 1. Redis 支持更丰富的数据类型（支持更复杂的应用场景）。Redis 不仅仅支持简单的 k/v 类型的数据，同时还提供 list，set，zset，hash 等数据结构的存储。Memcached 只支持最简单的 k/v 数据类型。
> 2. Redis 支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用,而 Memecache 把数据全部存在内存之中。
> 3. Redis 有灾难恢复机制。因为可以把缓存中的数据持久化到磁盘上。
> 4. Redis 在服务器内存使用完之后，可以将不用的数据放到磁盘上。但是，Memcached 在服务器内存使用完之后，就会直接报异常。
> 5. Memcached 没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据；但是 Redis 目前是原生支持 cluster 模式的。
> 6. Memcached 是多线程，非阻塞 IO 复用的网络模型；Redis 使用单线程的多路 IO 复用模型。（Redis 6.0 引入了多线程 IO ）
> 7. Redis 支持发布订阅模型、Lua 脚本、事务等功能，而 Memcached 不支持。并且，Redis 支持更多的编程语言。
> 8. Memcached 过期数据的删除策略只用了惰性删除，而 Redis 同时使用了惰性删除与定期删除。

Redis用到哪些命令（腾讯TEG一面）

Redis有哪些数据结构？（淘宝一面，今日头条测开二面）

> - STRING 字符串
> - LIST 列表
> - SET 无序集合
> - HASH 包含键值对的无序散列表
> - ZSET 有序集合

有序集合有了解嘛？跳表数据结构有了解嘛？（淘宝一面）

> 跳跃表是有序集合的底层实现之一。
> 
> 跳跃表是基于多指针有序链表实现的，可以看成多个有序链表。
> 
> 在查找时，从上层指针开始查找，找到对应区间之后再到下一层去查找。
> 
> 与红黑树灯平衡树相比，跳跃表具有以下优点：
> 
> - 插入速度非常快速，因为不需要进行旋转等操作来维护平衡性
> - 更容易实现
> - 支持无锁操作
> 
> 跳跃表每个节点的结构
> 
> ```c
> struct zslnode{
>     string value;
>     double score;
>     zslnode*[] forwards;    // 多层连接指针
>     zslnode*[] backward;    //回溯指针
> }
> ```

从zset删除数据的时间复杂度（腾讯云二面）

> [有序集(Sorted Set)](https://redis.readthedocs.io/en/2.4/sorted_set.html)
> 
> - 查找任意数据的时间复杂度O(logN)
> 
> - 插入数据的时间复杂度O(logN)
> 
> - 跳表的删除O(logN)

Redis中AOF和RDB有什么区别？哪种方法更优？（淘宝一面，==腾讯CDG事务开发一面==）

> - RDB是一次全量备份，AOF日志是连续的增量备份；
> - 存储。RDB是内存数据的二进制序列化形式，而AOF日志记录的是修改内存数据的指令；
> - 数据丢失。RDB往往会造成一定时间的数据丢失
> - 恢复速度。RDB快照恢复速度非常快，而AOF由于需要处理巨大的写入会影响Redis性能

Redis的分布式（==Lazada一面==）

> 一、CAP理论
> 
> - **C** - **C**onsistent ，一致性
> - **A** - **A**vailability ，可用性
> - **P** - **P**artition tolerance ，分区容忍性
> 
> 分布式系统的节点往往都是分布在不同的机器上进行网络隔离开的，这意味着必然会有网络断开的风险，这个网络断开的场景的专业词汇叫着「**网络分区**」。
> 
> 在网络分区发生时，两个分布式节点之间无法进行通信，我们对一个节点进行的修改操作将无法同步到另外一个节点，所以数据的「一致性」将无法满足，因为两个分布式节点的数据不再保持一致。除非我们牺牲「可用性」，也就是暂停分布式节点服务，在网络分区发生时，不再提供修改数据的功能，直到网络状况完全恢复正常再继续对外提供服务。
> 
> 2、最终一致
> 
> Redis 的主从数据是异步同步的，所以分布式的 Redis 系统并不满足「**一致性**」要求。
> 
> 当客户端在 Redis 的主节点修改了数据后，立即返回，即使在主从网络断开的情况下，主节点依旧可以正常对外提供修改服务，所以 Redis 满足「**可用性**」。
> 
> Redis 保证「**最终一致性**」，从节点会努力追赶主节点，最终从节点的状态会和主节点的状态将保持一致。如果网络断开了，主从节点的数据将会出现大量不一致，一旦网络恢复，从节点会采用多种策略努力追赶上落后的数据，继续尽力保持和主节点一致。
> 
> 3、主从同步
> 
> 类似于RDB快照和AOF

### 5.3 ElasticSearch

为什么要使用ES，MySQL索引和ES倒排索引的区别，ES的分词（深信服一面，腾讯TEG一面，==Lazada一面==，==Lazada二面==）

> [大白话告诉你倒排索引是个啥](https://zhuanlan.zhihu.com/p/112136054)
> 
> 一、正排索引
> 
> 正排索引就是数据库表，他通过id和数据进行关联。
> 
> 我们可以通过搜索id，来获得相应的数据，也能删除数据。你买了一本书，书的目录其实也是正排搜索。
> 
> 假设现在我要搜`苹果`俩字，那么他会对这张表格中每一行的数据做匹配，去查找一下，是否包含`苹果`这两个字，从第一条匹配到最后一条，如果一张表中数据量不多，几万，十几万，那么问题不大，但是一旦数据量有上百万，上千万，那么全表扫描这种的搜索性能就会有影响。
> 
> - 优点：使用起来方便，原理也简单，比较入门
> 
> - 缺点：检索效率低下，适合简单场景使用，比如传统项目，数据量较小的项目。不支持分词搜索。
> 
> 二、倒排索引
> 
> 倒排索引会把文档内容进行分词，比如`苹果公司发布iPhone`是一个文档数据，当我们把他存入到搜索引擎中去的时候，会有一个文档id，这个文档id就类似于数据库主键。但是这文档存储的时候和数据库不一样，他会进行一个分词，每一个词汇都会和文档id关联起来，可以根据词汇来找到所有出现的id列表
> 
> - 优点：搜索更快，耗时短，用户体验高，精装度也高
> 
> - 缺点：维护成本高，索引新建后要修改，必须先删除，前期需要很好地规划
> 
> 三、分词
> 
> 将文本转换为一系列单词的过程，也可以叫文本分析

ES的分布式特性（==Lazada一面==）

> - 将文档分区到不同的分片(shards)中，它们可以存在于一个或多个节点中；
> - 将分片均匀地分配到各个节点，对索引和搜索做负载均衡；
> - 冗余每一个分片，防止硬件故障造成数据丢失；
> - 将集群中任意一个节点（协调节点）上的请求路由到相应数据所在的节点；
> - 无论是增加节点，还是移除节点，分片都可以做到无缝的扩展和迁移；

### 5.4 Kafka

kafka的使用场景（今日头条测开二面，==百度一面==）

> [典型应用场景](https://support.huaweicloud.com/productdesc-kafka/kafka-scenarios.html)
> 
> - 异步通信
> - 错峰流控与流量削峰
> - 日志同步

MQ发送消息的模式？（腾讯CDG一面）

> - 点对点模式。（一对一，消费者主动拉取数据，消息收到后消息清除）
> - 发布/订阅模式。（一对多，消费者消费数据之后不会清除消息）

MQ丢包怎么处理？（腾讯CDG一面，==百度一面==）

> 一、生产者丢失消息的情况
> 
> 生产者(Producer) 调用`send`方法发送消息之后，消息可能因为网络问题并没有发送过去。
> 
> 所以，我们不能默认在调用`send`方法发送消息之后消息发送成功了。为了确定消息是发送成功，我们要判断消息发送的结果。但是要注意的是 Kafka 生产者(Producer) 使用 `send` 方法发送消息实际上是异步的操作，我们可以通过 `get()`方法获取调用结果，但是这样也让它变为了同步操作。如果消息发送失败的话，我们检查失败的原因之后重新发送即可！
> 
> 二、消费者丢失消息的情况
> 
> 我们知道消息在被追加到 Partition(分区)的时候都会分配一个特定的偏移量（offset）。偏移量（offset)表示 Consumer 当前消费到的 Partition(分区)的所在的位置。Kafka 通过偏移量（offset）可以保证消息在分区内的顺序性。
> 
> 当消费者拉取到了分区的某个消息之后，消费者会自动提交了 offset。自动提交的话会有一个问题，试想一下，当消费者刚拿到这个消息准备进行真正消费的时候，突然挂掉了，消息实际上并没有被消费，但是 offset 却被自动提交了。
> 
> 解决办法也比较粗暴，我们手动关闭自动提交 offset，每次在真正消费完消息之后再自己手动提交 offset 。 但是，细心的朋友一定会发现，这样会带来消息被重新消费的问题。比如你刚刚消费完消息之后，还没提交 offset，结果自己挂掉了，那么这个消息理论上就会被消费两次。
> 
> 三、Kafka弄丢消息的情况
> 
> 我们知道 Kafka 为分区（Partition）引入了多副本（Replica）机制。分区（Partition）中的多个副本之间会有一个叫做 leader 的家伙，其他副本称为 follower。我们发送的消息会被发送到 leader 副本，然后 follower 副本才能从 leader 副本中拉取消息进行同步。生产者和消费者只与 leader 副本交互。可以理解为其他副本只是 leader 副本的拷贝，它们的存在只是为了保证消息存储的安全性。
> 
> 试想一种情况：假如 leader 副本所在的 broker 突然挂掉，那么就要从 follower 副本重新选出一个 leader ，但是 leader 的数据还有一些没有被 follower 副本的同步的话，就会造成消息丢失。
> 
> 1、**设置 acks = all**
> 
> 解决办法就是我们设置 **acks = all**。acks 是 Kafka 生产者(Producer) 很重要的一个参数。
> 
> acks 的默认值即为1，代表我们的消息被leader副本接收之后就算被成功发送。当我们配置 **acks = all** 代表则所有副本都要接收到该消息之后该消息才算真正成功被发送。
> 
> 2、**设置 replication.factor >= 3**
> 
> 为了保证 leader 副本能有 follower 副本能同步消息，我们一般会为 topic 设置 **replication.factor >= 3**。这样就可以保证每个 分区(partition) 至少有 3 个副本。虽然造成了数据冗余，但是带来了数据的安全性。
> 
> 3、**设置 min.insync.replicas > 1**
> 
> 一般情况下我们还需要设置 **min.insync.replicas> 1** ，这样配置代表消息至少要被写入到 2 个副本才算是被成功发送。**min.insync.replicas** 的默认值为 1 ，在实际生产中应尽量避免默认值 1。
> 
> 但是，为了保证整个 Kafka 服务的高可用性，你需要确保 **replication.factor > min.insync.replicas** 。为什么呢？设想一下假如两者相等的话，只要是有一个副本挂掉，整个分区就无法正常工作了。这明显违反高可用性！一般推荐设置成 **replication.factor = min.insync.replicas + 1**。
> 
> 4、**设置 unclean.leader.election.enable = false**
> 
> > **Kafka 0.11.0.0版本开始 unclean.leader.election.enable 参数的默认值由原来的true 改为false**
> 
> 我们最开始也说了我们发送的消息会被发送到 leader 副本，然后 follower 副本才能从 leader 副本中拉取消息进行同步。多个 follower 副本之间的消息同步情况不一样，当我们配置了 **unclean.leader.election.enable = false** 的话，当 leader 副本发生故障时就不会从 follower 副本中和 leader 同步程度达不到要求的副本中选择出 leader ，这样降低了消息丢失的可能性。

Kafka如何搭建生产者和消费者（==腾讯CDG事务开发一面==）

Kafka的零拷贝（==百度一面==）

> [图解Kafka的零拷贝技术到底有多牛？](https://cloud.tencent.com/developer/article/1421266)
> 
> - DMA，全称叫Direct Memory Access，一种可让某些硬件子系统去直接访问系统主内存，而不用依赖CPU的计算机系统的功能
> 
> - 有了DMA后，就可以实现绝对的零拷贝了，因为网卡是直接去访问系统主内存的

消费者组（==腾讯CDG事务开发一面==）

> [怎么理解 Kafka 消费者与消费组之间的关系?](https://segmentfault.com/a/1190000039125247)
> 
> - 如果所有的消费者都隶属于同一个消费组，那么所有的消息都会被均衡地投递给每一个消费者，即每条消息只会被一个消费者处理，这就相当于点对点模式的应用。
> - 如果所有的消费者都隶属于不同的消费组，那么所有的消息都会被广播给所有的消费者，即每条消息会被所有的消费者处理，这就相当于发布/订阅模式的应用。

Kafka的分布式（==Lazada一面==，==百度一面==）

> ![Kafka Topic Partition](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/KafkaTopicPartitioning.png)
> 
> 比较重要的几个概念
> 
> 1. **Producer（生产者）** : 产生消息的一方。
> 2. **Consumer（消费者）** : 消费消息的一方。
> 3. **Broker（代理）** : 可以看作是一个独立的 Kafka 实例。多个 Kafka Broker 组成一个 Kafka Cluster。
> 
> 每个 Broker 中又包含了 Topic 以及 Partition 这两个重要的概念：
> 
> - **Topic（主题）** : Producer 将消息发送到特定的主题，Consumer 通过订阅特定的 Topic(主题) 来消费消息。
> - **Partition（分区）** : Partition 属于 Topic 的一部分。一个 Topic 可以有多个 Partition ，并且同一 Topic 下的 Partition 可以分布在不同的 Broker 上，这也就表明一个 Topic 可以横跨多个 Broker 。
> - **Replica（副本）**：分区（Partition）中的多个副本之间会有一个叫做 leader 的家伙，其他副本称为 follower。我们发送的消息会被发送到 leader 副本，然后 follower 副本才能从 leader 副本中拉取消息进行同步。
> 
> **Kafka 的多分区（Partition）以及多副本（Replica）机制有什么好处呢？**
> 
> 1. Kafka 通过给特定 Topic 指定多个 Partition, 而各个 Partition 可以分布在不同的 Broker 上, 这样便能提供比较好的并发能力（负载均衡）。
> 2. Partition 可以指定对应的 Replica 数, 这也极大地提高了消息存储的安全性, 提高了容灾能力，不过也相应的增加了所需要的存储空间。

### 5.5 Spring

Spring框架的IoC容器？（华为Cloudbu一面，淘宝一面）

> - IOC容器具有依赖注入的功能，它可以创建对象，IOC负责实例化、定位、配置应用程序中对象及建立这些对象之间的依赖。
> - 通常new一个实例，控制权由程序员控制，而“控制反转”是指new实例的工作不由程序员来做而是交给spring容器来做

Spring注入有哪些方式？怎么将依赖建立起来？（华为Cloudbu一面，淘宝一面）

> - 基于构造函数的依赖注入。当容器调用带有一组参数的类构造函数时，基于构造函数的DI就完成了，其中每个参数代表一个对其他类的依赖。
> - 基于设置函数setter的依赖注入。当容器调用一个无参的构造函数或一个无参的静态factory方法来初始化bean后，通过容器在bean上调用setter函数，基于setter函数的依赖注入就完成了。
> - 基于注解的依赖注入。使用相关类，方法或者字段声明的注解，将bean配置移动到组件类本身。

Spring中AOP的实现？面向接口、面向类是怎么实现的？（华为Cloudbu一面，淘宝一面，阿里3一面，==Lazada二面==）

> AOP(Aspect-Oriented Programming:面向切面编程)能够将那些与业务无关，却为业务模块所共同调用的逻辑或责任（例如事务处理、日志管理、权限控制等）封装起来，便于减少系统的重复代码，降低模块间的耦合度，并有利于未来的可拓展性和可维护性。
> 
> - 委托类
>   - 需要在实现类上加上`@Aspect`的注解，还需要通过`@Pointcut`注解来申明"切点"，即委托类和委托方法的路径
>   - 有了这些信息就足够获取委托类了。这里充分用到Java反射，先找到包含`@Aspect`注解的类，然后找到该类下的`@Pointcut`注解，读取所定义的委托类和委托方法注解，就完全能拿到委托类对象
> - 代理类
>   - 因为我们使用的是动态代理，这里的代理类可以被替换成`代理方法`。同样，我们在`@Aspect`注解的类中，用`@Around` `@Before` `@After`修饰的方法，就是我们想要的代理方法。
> - 总结
>   - 我们可以通过`BeanFactoryPostProcessor`的实现类，完成对所有`BeanDefinition`的扫描，找出我们定义的所有的切面类，然后循环里面的方法，找到切点、以及所有的通知方法，然后根据注解判断通知类型（也就是前置，后置还是环绕），最后解析切点的内容，扫描出所有的目标类。这样就获取了`委托类` 和 `代理方法`。
>   - 现在`委托类` 和 `代理方法` 都有了，我们知道在动态代理模式中，最终的目的是将委托类的方法执行，替换成代理类的方法执行。但是在Spring中，我们是感知不到`代理类`的，我们在代码中还是调用原`委托类`的方法，那么Spring框架是如何神不知鬼不觉地将`委托类`替换成`代理类`的呢？
>   - 在Bean的生命周期中，Bean在初始化前后会执行`BeanPostProcessor`的方法。可以把它理解成一个增强方法，可以将原始的Bean经过“增强”处理后加载到Ioc容器中。这就是一个天然的代理方法，原始的Bean就是`委托类`，在此处实现代理方法生成代理类，再将代理类加载进Ioc容器。

servlet的类继承层次（腾讯云二面）
